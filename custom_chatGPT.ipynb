{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dd15f7-86b3-4eac-96dc-aac162560455",
   "metadata": {},
   "source": [
    "# Custom ChatGPT with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c8fd1-3216-4718-bf6c-ae641fa953b5",
   "metadata": {},
   "source": [
    "The official ChatGPT web app is great, but it might not be tailored to our specific needs. A custom ChatGPT app provides several advantages over using the prebuilt ChatGPT app.\n",
    "## Advantages of Using a Custom ChatGPT App\n",
    "**1)Increased Security:-** You can implement security measures, designed to meet your company's needs This means, you can control who has access to the app and how the app is used. This can help to protect your company's data. for example, you could implement two-factor authentication, restrict access to the app based on the user's role, or encrypt the stored data. You can also use a firewall to block unauthorized access to the app.\n",
    "\n",
    "**2)Improved User Experience:-** You can personalize the app according to your user needs. This means you can customize the app's layout, branding, and functionality to match your company's style and requirements\n",
    "\n",
    "**3)Greater Flexibility** to add new features or functionality as needed. Because we are not limited to the features available in the pre-built ChatGPT app. for example, you can use another model like Llama2(Meta) which is open source, or another one from the Hugging Face Hub. You can add new features to the app as your needs change, or you can modify the app's functionality to meet the specific needs of your users. for example, you can add a feature that allows users to track their progress on a project, or you can connect a generative text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adde0d-6611-4477-94ce-93dac9f97af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f60c9e-84dc-44dc-8a18-ee400f3004a9",
   "metadata": {},
   "source": [
    "In this app, we will use a chat model, Which means,the exchange of messages back and forth. Instead of PromptTemplate, We will use **ChatPromptTemplate**.\n",
    "\n",
    "We will add memory to our custom ChatGPT app. Memory is the ability to store information about past interactions with the LLM. We can create an instance of memory and incorporate it into a chain. The general goal of memory is to store some data inside the chain, so we can use it in the future when we run the same chain again. LangChain provides us with many types of memory. Each has its own parameters, its own return types, and is useful in different scenarios.\n",
    "\n",
    "For Chat models, we will use **ConversationBufferMemory** class. Its goal is to keep a list of all the chat messages in a buffer and pass those into the prompt template.\n",
    "\n",
    "**MessagesPlaceholder** is a prompt template that allows you to insert a list of history messages during formatting, and it is useful when you are using ConversationBufferMemory to store the chat history.\n",
    "\n",
    "if we take a look at the original ChatGPT app or Bard, we can notice that all past conversations will be saved there. If we interrupt the session and return later, we can continue the same conversation. We use a special langchain component named **FileChatMessageHistory**to implement this feature in our LLM app. We save the chat messages to a file before exiting, and when starting the app again, it will load the previous messages from that file. Note:- Instead of a file, we can save the chat history to more robust external databases too, like Postgres, MongoDB, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e00e5-973a-4ac5-bfc5-f84147e07c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your prompt:  where is it located?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: What is the capital of india\n",
      "AI: The capital of India is New Delhi.\n",
      "Human: same famous tourist places over there?\n",
      "AI: Yes, there are many famous tourist places in New Delhi, such as the Red Fort, India Gate, Qutub Minar, Lotus Temple, Akshardham Temple, Humayun's Tomb, and Rashtrapati Bhavan (the President's Residence). Delhi also has vibrant markets like Chandni Chowk and Connaught Place where you can shop and experience local culture.\n",
      "Human: where is it located?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "New Delhi is located in the northern part of India. It is part of the National Capital Territory of Delhi, which is surrounded by the state of Haryana on three sides and Uttar Pradesh on the east.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#1. imports\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature = 1)\n",
    "\n",
    "\n",
    "history=FileChatMessageHistory('chat_history456.json')  #All the chat messages history will be saved to this file\n",
    "\n",
    "\n",
    "#2 memory object\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = 'chat_history', # It means that the chat memory will be stored in the chat_history key. This key will be used in ChatPromptTemplate.\n",
    "    chat_memory=history,\n",
    "    return_messages = True   # It tells the memory to return the chat history as a list of messages instead of a string.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variable = ['content'],\n",
    "    messages=[\n",
    "    SystemMessage(content='You are a chatbot having a conversation with a human.'), \n",
    "    MessagesPlaceholder(variable_name='chat_history'), #where the memory will be stored. It has to be added here..after the SystemMessage and before the HumanMessagePromptTemplate.\n",
    "    HumanMessagePromptTemplate.from_template('{content}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,  # adding memory to the chain\n",
    "    verbose =True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response['text'])\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdace354-51e3-495e-ab56-d107b2b4f2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
